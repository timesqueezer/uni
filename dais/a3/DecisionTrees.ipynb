{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Decision Trees II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this task, we want to explore how to program decision trees in Python. We will use some popular packages and libraries that are specifically developed for this machine learning method, but also manually design some of the functions needed to set up a decision tree classifier ourselves. \n",
    "\n",
    "Before you start working on this task, check out the [Tree](https://scikit-learn.org/stable/modules/tree.html) module from the [Scikit-Learn](https://scikit-learn.org/stable/index.html) library and get familiar with some functions this module provides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't installed the correct version 0.24.1 of scikit-learn yet, just run the following line of code to properly install it on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn==0.24.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Obtaining the **Iris Plants** dataset\n",
    "Let's use another interesting dataset this time: The [Iris Plants Dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#iris-dataset) provided by scikit-learn is used to classify three different types of iris flower. Similar to the Boston Housing Prices dataset, this one is fully preprocessed and ready for analyzing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = iris.feature_names\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = iris.target_names\n",
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as you can see, the class names are the Latin names for each type of iris flower. But let's continue by taking a look at the dataset description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Automated Decision Tree Classification\n",
    "We can now get the DecisionTreeClassifier and train it on our feature and target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "X, y = iris.data, iris.target\n",
    "dt = tree.DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
    "dt = dt.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, now it's time to plot our tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.plot_tree(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't look too good. We need a bigger figure size and don't want to see this unreadable text. So let's use ```matplotlib``` to adjust the size of the plot and enter some parameters to the ```plot_tree()``` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(30,10))\n",
    "_ = tree.plot_tree(dt, feature_names=features, class_names=classes, filled=True, fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way ```filled=True``` as described in the ```plot_tree``` [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html) is responsible for coloring the decision tree according to indicate majority class for classification, extremity of values for regression, or purity of node for multi-output.\n",
    "\n",
    "So now we know how to automatically create a decision tree using existing Python libraries. But since we still haven't taken a look behind the curtains of such automated functions, we want to write some decision tree classifier ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Attribute Splitting with ID3 Algorithm\n",
    "\n",
    "In the lecture, you got to know two criteria for attribute split, i.e. information gain (entropy) and gini impurity. Now it's time to write some functions that determine the next attribute to split on according to the entropy criterion. **Your part will be to implement the four helper functions below**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first function ```entropy(...)``` takes an array of values (usually labels) and outputs the entropy (or _info_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(value_arr):\n",
    "    # your code here\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second function ```avg_info(...)``` takes an array of values of an attribute (that is usually an entire column in a dataset if each attribute represents one column) and an array of labels of the same length and outputs the average information of that attribute. This means, the function has to be invoked once for each attribute in a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_info(attr_values, labels):\n",
    "    # your code here\n",
    "    return avg_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the overall entropy (as integer/float/...) and the average information of all attributes (as an array), the function ```info_gain(...)``` calculates the information gain of all attributes and returns the corresponding array. This means, we call this function once, not for all attributes separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_gain(info, attr_info):\n",
    "    # your code here\n",
    "    return gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, in the function ```get_split_attr(...)``` the index position of the attrib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_attr(gain_arr):\n",
    "    # your code here\n",
    "    return attr_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's some code to test your functions (you don't have to change anything here). All steps are basically following the equations in slide 34 of the [decision tree slide set](https://lernen.min.uni-hamburg.de/pluginfile.php/164416/mod_resource/content/2/L07%20Simple%20Decision%20trees_2021.pdf). So try to understand what calculations have to be done there and this task will be a breeze. ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# TEST CODE / MAIN FUNCTION\n",
    "#\n",
    "\n",
    "\n",
    "# Step 1: Calculate Information (Entropy)\n",
    "info = entropy(iris.target)\n",
    "\n",
    "# Step 2: Calculate Average Information of all Attributes\n",
    "attr_info = [avg_info(attr, iris.target) for attr in iris.data.T]\n",
    "\n",
    "# Step 3: Calculate Information Gain\n",
    "gain = info_gain(info, attr_info)\n",
    "\n",
    "# Step 4: Determine Split Attribute based on Information Gain\n",
    "attr_pos = get_split_attr(gain)\n",
    "attr_name = iris.feature_names[attr_pos]\n",
    "\n",
    "# Step 5: Some fancy output for debugging\n",
    "print('The next attribute to use for splitting is {}.'.format(attr_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Hint: Does the print-statement output the same attribute that has been split on in the decision tree above right in the root? That should be a good sign!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (A1_PythonIntro)",
   "language": "python",
   "name": "pycharm-2b27f6e8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
