%!TEX encoding = UTF-8 Unicode
% ================================================================================
\documentclass[
    fontsize=12pt,
    headings=small,
    parskip=half,           % Ersetzt manuelles Setzen von parskip/parindent.
    bibliography=totoc,
    numbers=noenddot,       % Entfernt den letzten Punkt der Kapitelnummern.
    open=any,               % Kapitel kann auf jeder Seite beginnen.
%   final                   % Entfernt alle todonotes und den Entwurfstempel.
    ]{scrreprt}
% ===================================Praeambel==================================
\include{stylesvs}

%\addbibresource{expose.bib}

% ===================================Dokument===================================

\title{How can federated learning be utilized to implement a decentralized and privacy focused bot detection system for websites?}
\author{Matz-Jona Radloff}
% \date{01.01.2015} % Falls ein bestimmtes Datum eingesetzt werden soll, einfach
                    %  diese Zeile aktivieren.

\begin{document}

\begin{titlepage}
\begin{center}\Large
	\vfill
    Exposé for Bachelor Thesis
	\vfill
	\makeatletter
	{\Large\textsf{\textbf{\@title}}\par}
	\makeatother
	\vfill
    submitted by
	\par\bigskip
	\makeatletter
	{\@author} \par
	\makeatother
	Matriculation number 6946325 \par
	Study Program: Computer Science
	\vfill
	\makeatletter
	submitted on {\@date}
	\makeatother
	\vfill
	Supervisor: August See\par
	First reviewer: Prof. Dr. Mathias Fischer \par
	Zweitgutachter: N.N.
\end{center}
\end{titlepage}


\chapter*{Abstract}

Malicious use of automated bots present an increasing risk to applications in the web. Existing solutions do either not perform well, are not accessible to many providers due to high cost, or disregard modern privacy standards. This work aims to explore the state-of-art as well as provide a proof-of-concept for a basic system that incorporates all of the above criterions. These concerns will be addressed by implementing an open-source library that uses federated learning which performs well enough to be considered a realistic choid within currently available alternatives.

\tableofcontents

\chapter{Motivation}

In this work, the term bot is referring to software that is automatically performing HTTP(S) requests with the intent of harming the target or reaching another malicous goal. While this threat is nothing new to the web the attack surface has grown significantly over the past year. Especially the increased usage of web interfaces in poorly secured IoT devices and the trend to (re-)implement software as web applications is resposible for this.

Bot attacks can have several goals. DoS attacks aim to overload the target's infrastructure such that it becomes inaccessible for normal use. Carding and Credential stuffing refers to performing payment or login requests to find working credit card numbers and credentials usually obtainend from a data breach. Data scrapers download the website data and can use the data for malicious purposes, e.g. damage SEO or violate copyrights. Content spam includes inserting malicious or polluting data on platforms that allow user generated content. Scalping or inventory hoarding of shopping items can artificially raise prices, damage brands, generate false market forces and create a bad customer experience.

Recent studies show that of 2020's internet traffic, 25.6\% was fraudulent and automatically generated \cite{BAD_BOT_REPORT2020} \cite{BAD_BOT_REPORT2021}. They also show that both the percentage of bot traffic in general as well as malicious bot traffic has increased over time.

Most of the above attacks need to trick the webserver and application backends into performing the request as if it had been initiated by a human. Instead of combating the resulting issues separately, bot detection could potentially mitigate many at once.

A complication in this problem space is the, often desired, requirement for non-malicious bots to be granted normal access. The most prominent example are scraper bots from search engines that need to periodically request websites to build their search indices. A common technique to exploit this requirement is trying to emulate known bot signatures from large search engines, e.g. Googlebot \cite{TODO}.

Many website operators tend to use solutions that are easy to integrate and perform well. This requires embeding external software which collects user data and sends it to different servers of the software vendors. These often do not disclose what exactly happens to the user data and website operators open themselves to additional threats in case of a data breach. Depending on the operating countries of both the websites and software vendors, data privacy regulation might also not allow sharing user data at all or require the operator to document the concrete data transfer in a very detailed and legally complicated way, e.g. in countries falling under the GDPR \cite{GDPR}. Because of the above reasons it is desirable to either employ self-hosted software or use a solution that does not require user data transfers.

\todo{machine learning motivation}

A promising technique that combines both machine learning and respecting modern privacy standards is federated learning \cite{DBLP:journals/corr/KonecnyMR15} \cite{DBLP:journals/corr/KonecnyMRR16}.

\chapter{Method}

This exposé and the thesis based on it initially explore related works and the state of the art in the field of applying machine learning and especially federated learning to bot detection. Next a system architecture of a potential software is proposed as a proof of concept. This includes decisions about what existing software libraries are suitible for integration, how and where the software needs to run in practice, how federated learning will be integrated, and what data exactly is most appropriate to be used in the learning algorithm. A hypothesis is made that this system is practically feasible. The evaluation of the claims is being done in terms of performance, privacy and comparability to existing solutions.

\chapter{Related Work}

\section{Proprietary Solutions}

\url{https://datadome.co} \\
\url{https://www.perimeterx.com/products/bot-defender/} \\
\url{https://www.imperva.com/products/advanced-bot-protection-management/} \\
\url{https://www.fastly.com/products/cloud-security/bot-protection} \\
\url{https://www.cloudflare.com/products/bot-management/} \\


\section{Scientific Work}

The paper "FLEAM: A Federated Learning Empowered Architecture to Mitigate DDoS in Industrial IoT" \cite{LiJi2021} introduces a federated learning approach similar to the goals of this work but differs in the specific use case and implementation. Their system focusses on the detection of IoT (Internet of Things) devices which are easily hacked and turned into zombies. These zombies are commonly used in DDoS (Distributed Denial of Service) attacks which their strategy tries to make not feasible to perform. They also develop their own iterative model averaging based method "gated recurrent unit" (GRU) which is optimized for their specific use case.

Other works related to DDoS mitigation that also incorporate machine learning include Farivar \emph{et al.} \cite{FaFa2020}, Liu \emph{et al.} \cite{8594641} and Hussain \emph{et al.} \cite{9000893}.

Many of the existing state of the art solutions use the actual IP network traffic to extract relevant information to be used as input parameters for their models. The browser environment in the web offers a much greater amount and variety of user information that can be very useful to differentiate between a valid user and a malicious bot and is not accessible on the IP packet level.

The work of \cite{PETS2021} outlines the problems and privacy-realted concerns really well and tries to solve a very similar problem but focusses on mobile devices and does not incorporate federated learning. Instead the authors perform the actual check on a pre-trained model on the user's device and generate a proof which is verified on a server.


\chapter{Proposed Architecture}

This work aims to incorporate an existing library that provides federated learning capabilities and can be run either on the client's browser or the websites' servers.

The proof of concept is intended to show the feasibility of implementing such a system while taking the specified constraints into account. It will include multiple website instances that all use the bot detection library and a primary server that holds the model, facilitates communication to and from the website instances, and incorporates learned updates.

To avoid having to use user data at all, the learning system could only work on request metadata on the web server or backend level. In this case the available data would limited to combinations of IP data, such as addresses and timing data, as well as HTTP request metadata which, in most cases, contains pre-defined data that can be easily faked by bots. This work assumes that dynamic user data is needed in order to properly distinguish between human- and bot-initiated requests. It also needs to determine whether it suffices to gather data in the website's backend or frontend only or if both soruces are needed. For now all possiblities will remain open and potential existing software will be evaluated.

At the same time a friction-less system is desired that uses already existing data instead of requiring additional user input, e.g. solving a puzzle or performing object detection.

\section{Website Instance Structure}

To be determined is whether to perform the actual training of the model in the client's browser or the websites' backend server. The former variant has the advantage of the client data never leaving the own browser which makes a strong case for privacy concerns. It also allows websites that don't use or don't have access to a backend to use this system. Its disadvantages are potentially high performance penalties, e.g. if the training code needs to run on older hardware which also might affect the actual website's performance. Another potential risk is the exposure of the training logic and model which attackers might use to reverse engineer the system and find a way to disguise themselves as non-malicious clients.

Analysis of available libraries revealed only three potential candidates for the browser-based approach:

\begin{enumerate}
	\item Experimental and unmaintained library for TensorFlow.js \cite{PAIRFL2019}
	\item A github user's attempt \cite{SaFL2019} to solve this problem following a discussion in TensorFlow's github issues.
	\item Implementing an own solution on top of a browser-based ML library.
\end{enumerate}

The amount and quality of libraries suitable for the server-side approach seems to be much higher. A selection of the most popular libraries includes the following projects:

\begin{enumerate}
	\item \url{https://github.com/IBM/federated-learning-lib}
	\item \url{https://fedml.ai/}
	\item \url{https://flower.dev/}
	\item \url{https://www.tensorflow.org/federated/federated_learning}
\end{enumerate}

Weighing the advantages and disadvantages of both solutions against each other leads to the choice of running the federated learning client on the website's backends. Both the potentially increased performance and non-exposure of the learning logic and model are the biggest factors in this choice. Privacy-concious website operators need to either choose a hoster that provides access to the backend software, or host the application backend themselves such that control over the user data can be guaranteed. Due to this the requirement to install the software in the backend is reasonable.

\section{Backend Technology Stack}

As python is the de-facto language of choice for machine learning applications and has both great available projects for web applications and is well suitable for rapid prototyping, it is also used for both the website instances' and primary server's backends.

The specific software stack consists of a python application using the Flask microframework which serves a REST-style API and the static frontend files in the website instances.


\section{Federated Learning}

A big part of the system design is going to be the choice of user data and transformations performed on the same to be used in the model's parameter space.


\chapter{Empirical Study}

\section{Performance}

To be determined is whether the proposed architecture actually works with the goal.



\begin{raggedright}
  \printbibliography
\end{raggedright}

\end{document}
