%!TEX encoding = UTF-8 Unicode
% ================================================================================
\documentclass[
    fontsize=12pt,
    headings=small,
    parskip=half,           % Ersetzt manuelles Setzen von parskip/parindent.
    bibliography=totoc,
    numbers=noenddot,       % Entfernt den letzten Punkt der Kapitelnummern.
    open=any,               % Kapitel kann auf jeder Seite beginnen.
    final                   % Entfernt alle todonotes und den Entwurfstempel.
    ]{scrreprt}
% ===================================Praeambel==================================
\include{stylesvs}

\addbibresource{expose.bib}

% ===================================Dokument===================================

% \title{On using privacy preseving machine learning for\\decentralized web bot detection}
\title{On machine learning based bot detection using mouse dynamics and request metadata in a practical context}
\author{Matz-Jona Radloff}
% \date{01.01.2015} % Falls ein bestimmtes Datum eingesetzt werden soll, einfach
                    %  diese Zeile aktivieren.

\begin{document}

\begin{titlepage}
\begin{center}\Large
	\vfill
    Expos√© for Bachelor Thesis
	\vfill
	\makeatletter
	{\Large\textsf{\textbf{\@title}}\par}
	\makeatother
	\vfill
    submitted by
	\par\bigskip
	\makeatletter
	{\@author} \par
	\makeatother
	Matriculation number 6946325 \par
	Study Program: Computer Science
	\vfill
	\makeatletter
	submitted on {\@date}
	\makeatother
	\vfill
	Supervisor: August See\par
	First reviewer: Prof. Dr. Mathias Fischer \par
	Zweitgutachter: N.N.
\end{center}
\end{titlepage}


\chapter*{Abstract}

Malicious use of automated bots present an increasing risk to applications in the web. Existing solutions do either not perform well, are not accessible to many providers due to high cost, or disregard modern privacy standards. This work aims to provide a proof-of-concept for a basic system that incorporates all of the above criterions and compares different combinations of the system with and without federated learning and personal data.

\tableofcontents

\chapter{Motivation}

In this work, the term bot is referring to software that is automatically performing HTTP(S) requests with the intent of harming the target or reaching another malicous goal. While this threat is nothing new to the web the attack surface has grown significantly over the past year \cite{BAD_BOT_REPORT2020,BAD_BOT_REPORT2021}. Especially the increased usage of web interfaces in poorly secured IoT devices and the trend to (re-)implement software as web applications is responsible for this.

The usage of bots can have several goals. This thesis primarily focuses on detecting web-based bots that try to access or perform actions on websites but other and related attack types exist, for example:
DoS attacks aim to overload the target's infrastructure such that it becomes inaccessible for normal use. Carding and Credential stuffing refers to performing payment or login requests to find working credit card numbers and credentials usually obtainend from a data breach. Data scrapers download the website data and can use the data for malicious purposes, e.g. damage SEO or violate copyrights. Content spam includes inserting malicious or polluting data on platforms that allow user generated content. Scalping or inventory hoarding of shopping items can artificially raise prices, damage brands, generate false market forces and create a bad customer experience.

Recent studies show that of 2020's internet traffic 25.6\% was fraudulent and automatically generated \cite{BAD_BOT_REPORT2020} \cite{BAD_BOT_REPORT2021}. They also show that both the percentage of bot traffic in general and malicious bot traffic has increased over time.

Most of the above attacks need to trick the webserver and application backends into performing the request as if it had been initiated by a human. Instead of combating the resulting issues separately, bot detection could potentially mitigate many at once.

A complication in this problem space is the, often desired, requirement for non-malicious bots to be granted normal access. A prominent example are scraper bots used by search engines that need to request websites periodically to build their search indices. A common technique to exploit this requirement is trying to emulate known bot signatures from large search engines, e.g. Googlebot \cite{8421894}.

Many website operators tend to use solutions that are easy to integrate. This requires embeding external software which collects user data and sends it to servers of the software vendors. Closed source software does not allow to determine what exactly happens to the user data and website operators open themselves to additional threats in case of a data breach. Depending on the operating countries of both the websites and software vendors, data privacy regulation might also not allow sharing user data at all or require the operator to document the concrete data transfer in a very detailed and legally complicated way, e.g. in countries falling under the GDPR \cite{GDPR}. Because of the above reasons it is desirable to either employ self-hosted software or use a solution that does not require user data transfers.



\chapter{Related Work}

\section{Proprietary Solutions}

\url{https://datadome.co/} \\
\url{https://www.perimeterx.com/products/bot-defender/} \\
\url{https://www.imperva.com/products/advanced-bot-protection-management/} \\
\url{https://www.fastly.com/products/cloud-security/bot-protection} \\
\url{https://www.cloudflare.com/products/bot-management/} \\
\url{https://developers.google.com/recaptcha/docs/v3} \\
\url{https://www.hcaptcha.com/} \\


\section{Scientific Work}

The paper \cite{LiJi2021} introduces a federated learning approach similar to the goals of this work but differs in the specific use case and implementation. Their system focuses on the detection of IoT (Internet of Things) devices which are easily hacked and turned into zombies. These zombies are commonly used in DDoS (Distributed Denial of Service) attacks which their strategy tries to make not feasible to perform. They also develop their own iterative model averaging based method "gated recurrent unit" (GRU) which is optimized for their specific use case.

Iliou et al. \cite{10.1145/3339252.3339267} present a comparison of different machine learning algorithms and combinations of different attributes used in previous literature. The attributes comprise mostly of request metadata which would be suitible for a privacy-friendly bot detection system, for example the percentage of image requests or the number of total bytes per session. The authors split the bot data in their dataset into simple and advanced bots which is determined by whether the requests have a browser agent name and, in case they do, whether the IPs have shown malicious activity before. Their results show that different sets of attributes are performing best depending on the classification algorithm used. The best performing one is Random Forest although the paper concludes that using an ensemble classifier that averages over all used methods would be more stable. Additionally simple web bots can be detected very easily while detecting advanced bots is significantly harder, with areas under the ROC curve of $1.00$ and $0.64$ respectively. Especially in false positive intolerant use cases the performance of detecting advanced bots is too poor to be used in the real world. The authors conclude that future work would need to incorporate more advanced features which can not be easily simulated by bots.

The work of \cite{PETS2021} outlines the problems and privacy-realted concerns really well and tries to solve a very similar problem but focuses on mobile devices. The authors run a pre-trained machine learning model on the user's device. To avoid local changes to the model a cryptographic proof is generated that is verified on a server.

Among others, the works of Shen et al. \cite{6263955} and Antal et al. \cite{9111596} \cite{DBLP:journals/corr/abs-1810-04668} show the viability of using mouse and trackpad actions to verify the authenticity of users but privacy concerns often stand in the way of using such a method in practice.

Additionally Acien et al. \cite{Acien2020BeCAPTCHAMouseSM} show the feasibility of using biometric features for bot detection in general and propose new mouse trajectory synthesis methods as well as a GAN-based learning system that can distinguish between humans and bots with 93\% accuracy with only one mouse trajectory as input.


\chapter{Method}


\section{Concept}

The thesis explores how additional data, which might be personal user data, can potentially improve a machine learning system for bot detection. It also shows how federated learning can be used to allow using user data while mainting privacy.

For comparison a similar system to the proposed architecture of Iliou et al. \cite{10.1145/3339252.3339267} is implemented. The authors propose a machine learning based web bot detection framework which operates on request log data. They also identify which extracted features of the data perform the best in this context. Their method was tested on a year's worth of HTTP log data from MK-Lab's public web server\footnote{Multimedia Knowledge and Social Media Analytics Laboratory, \url{https://mklab.iti.gr/}}. The data includes IP addresses, request method, request path, referrers, user agent strings and timestamps.
Iliou et al.'s work is used as the basis for the machine learning system of this thesis because they compared the most promising features that have been proposed over 5 years prior to their publication (2019) and accumulated their findings in concise results.

Mouse, touchpad or touch gestures, which are not considered by Iliou et al. \cite{10.1145/3339252.3339267}, supplement the data in this work's approach. This type of data is harder to fake by attackers trying to emulate human behavior and would be classified as sensitive user data. It could not reasonably be used by a third party provider and would require explicit user consent \cite{GDPR}.

To avoid the complicated privacy-related implications federated learning, a technique that uses machine learning and respects modern privacy standards \cite{DBLP:journals/corr/KonecnyMR15} \cite{DBLP:journals/corr/KonecnyMRR16}, is used and compared against.


\section{Datasets}


\subsection{Human Data}

To realistically match request metadata and user mouse data a dataset is gathered from two websites in a user experiment. The websites have a basic blog-style structure with different information sections and user registration features. Their web servers log all relevant data to extract the features required for the metadata approach while a javascript library records mouse data and sends it to the site's backends for storage.

While no suitible datasets exist that include both request and mouse data, many publicly available datasets exist that contain valid user mouse movement and click data. If it is going to seem useful the experiments data could be augmented by these.
Shen et al.'s \cite{6263955} dataset contains mouse dynamics information from 28 users and 30 sessions per users which each contain around 3000 mouse events.
The DFL dataset \cite{9111596} includes 20-30 sessions of 21 users.
The Balabit Mouse Dynamics Challenge Data Set \cite{BALABIT_CHALLENGE} includes a few longer session and several shorter session which are meant to be used for training and testing respectively. For the purposes of bot detection, both can be used.

If needed, additional datasets are available, e.g.:
\footnote{\url{https://figshare.com/articles/dataset/Mouse_Behavior_Data_for_Static_Authentication/5619313}}
\footnote{\url{https://www.uvic.ca/ecs/ece/isot/datasets/?utm_medium=redirect&utm_source=/engineering/ece/isot/datasets/}}


\subsection{Bot Data}

To generate bot request data two different approaches are implemented by using the selenium-python \cite{SELENIUMPYTHON} library which runs automated actions in an actual browser environment. Instead of the standard chromedriver which runs a headless google chrome instance, undetected-chromedriver \cite{UNDETECTEDCHROMEDRIVER} is used which is a patched version that claims to not trigger many anti-bot services. If time allows it, both variants could be compared against, too.

The first, naive approach just performs the desired actions, e.g. scraping information or registering a user account. The second includes efforts to make the requests seem more human, for example by using randomized delays or requesting different pages before performing the actual target actions.

Mouse data labeled as bot input will be generated using two methods. The first naive approach interpolates linearly between the start and end point of randomly generated movements. The second method uses existing open source libraries (Probably one of \footnote{\url{https://github.com/AntoinePassemiers/Realistic-Mouse}},\footnote{\url{https://github.com/patrikoss/pyclick}}). More complicated simulations exist but their implementations are not publicly available. \cite{8275816} \cite{Nazar2003} The two methods used depict a reasonable choice of a basic attack that tries to evade detection by fake mouse movement.

Raw mouse data is segmented into mouse actions such as mouse move (MM) or mouse move and a click (point and click, PC) similar to \cite{9111596} and their previous paper \cite{DBLP:journals/corr/abs-1810-04668}. Multiple results can be averaged to increase detection performance.


\section{Machine Learning Model}

Many different machine learning models are suitable for binary classification. Hu et al. \cite{8275816} compare different classifiers in a context where mouse data is used. Random Forest and Multilayer Perceptron perform the best.
Iliou et al. \cite{10.1145/3339252.3339267} also show that Random Forest performs the best when using request metadata.
If time allows it, the thesis will compare multiple classification methods against each other and only use one otherwise.

Iliou et al. \cite{10.1145/3339252.3339267} ranked the best performing metrics for simple and advanced bots per classification algorithm. The following are listed for Random Forest and advanced bots:


\begin{enumerate}
	% \item Total number of HTTP HEAD requests issued during the session. (5)
	\item The percentage of HTTP requests that led to an HTTP 4xx code response. (7)
	\item The percentage of HTTP requests that requested a css file. (10)
	\item The percentage of HTTP requests that requested a JavaScript file. (11)
	% \item The number of the requested HTML files divided by the number of requested image files in a session. (12)
	% \item Boolean indicating if a session has at least one request with a known search engine refer. (14)
	\item The percentage of HTTP requested URLs that contain the previously requested URL as a subpart. (20)
	\item The total time (in seconds) between the first and the last HTTP request of the session. (21)
	\item 16 Depth SD Standard deviation of requested pages' depth (i.e. number of ‚Äô/‚Äô in URL path).
\end{enumerate}

Attribute 14 might not be suitible as the users in the experiment will be asked to access the websites directly. The websites are otherwise designed such that all attributes are meaningful.

The mouse data input features will consist of the (normalized) $x$- and $y$-coordinates as well as a time value for each mouse event. Additional features will be engineered similar to \cite{DBLP:journals/corr/abs-1810-04668}, such as mean, standard deviation, minimum and maximum value of path tangent, horizontal, vertical and overall velocity, acceleration, jerk, angular velocity. Additionally the type of action, length of the movement and time needed to complete the action will be used.

A system for pre-processing the different datasets will be developed.

The thesis will either use Tensorflow with and without federated learning or scikit-learn with/without Flower.
Tensorflow is one of the most widely used machine learning frameworks and the integration of its runtime into distributed learners in the form of website backends or even client devices is feasible. Scikit-learn and Flower seem to be more accessible but both approaches will be compared. The thesis does not include the actual integration into such systems. A simulated environment of multiple learner instances that have access to a secure communications channel will be developed.

\section{Evaluation}

The performance of the following scenarios will be evaluated and compared:

\begin{enumerate}
	\item Only request metadata, data from both websites, naive bot data
	\item Only request metadata, data from both websites, advanced bot data
	\item Request metadata and mouse data, data from both websites, naive bot data
	\item Request metadata and mouse data, data from both websites, advanced bot data
	\item Only request metadata, combined data by federated learning, naive bot data
	\item Only request metadata, combined data by federated learning, advanced bot data
	\item Request metadata and mouse data, combined data by federated learning, naive bot data
	\item Request metadata and mouse data, combined data by federated learning, advanced bot data
\end{enumerate}

The performance will be primarily quantified by bot detection rate. Additionally the accuracy will be compared by providing the confusion matrices and ROC curve visualizations of all scenarios. Especially the false positive rate will be weighed very highly as bot detection is false positive intolerant to not disturb the user experience.

\chapter{Time plan}

My planned steps in roughly two week intervals: \\

\begin{enumerate}
	\item Implement the basic websites and plan the experiment
	\item Start implementing data aggregator and feature extractor
	\item Implement the bot data generator (only naive types)
	\item Build the ML model, verify that it works and that it can classify the data correctly
	\item Compare the metadata and mouse data approaches
	\item Integrate Federated learning and build the simulated environment of multiple distributed learners
	\item Extend the bot data generation
	\item Formulate and perform the experiments to test the thesis' hypothesis
	\item Process and visualize the results
\end{enumerate}


\begin{raggedright}
  \printbibliography
\end{raggedright}

\end{document}
