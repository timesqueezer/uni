%!TEX encoding = UTF-8 Unicode
% ================================================================================
\documentclass[
    fontsize=12pt,
    headings=small,
    parskip=half,           % Ersetzt manuelles Setzen von parskip/parindent.
    bibliography=totoc,
    numbers=noenddot,       % Entfernt den letzten Punkt der Kapitelnummern.
    open=any,               % Kapitel kann auf jeder Seite beginnen.
    final,                   % Entfernt alle todonotes und den Entwurfstempel.
    table
]{scrreprt}

% ===================================Praeambel==================================
\usepackage[table]{xcolor}
\include{stylesvs}


\setlength{\arrayrulewidth}{0.2mm}
\setlength{\tabcolsep}{4pt}
% \renewcommand{\arraystretch}{1.5}
\renewcommand{\arraystretch}{1.2}


\addbibresource{thesis.bib}

% ===================================Dokument===================================

\begin{document}

% \title{On using privacy preseving machine learning for\\decentralized web bot detection}
\title{On Machine Learning Based Bot Detection Using Mouse Dynamics and Request Metadata in a Practical Context}
\author{Matz-Jona Radloff}
\date{19.08.2022} % Falls ein bestimmtes Datum eingesetzt werden soll, einfach
                    %  diese Zeile aktivieren.


\begin{titlepage}
\begin{center}\Large
    \vfill
    Bachelor Thesis
    \vfill
    \makeatletter
    {\Large\textsf{\textbf{\@title}}\par}
    \makeatother
    \vfill
    submitted by
    \par\bigskip
    \makeatletter
    {\@author} \par
    \makeatother
    Matriculation number: 6946325 \par
    Study Program: Computer Science
    \vfill
    \makeatletter
    submitted on {\@date}
    \makeatother
    \vfill
    Supervisor: August See\par
    First reviewer: Prof. Dr. Mathias Fischer \par
    Second reviewer: August See
\end{center}
\end{titlepage}


\chapter*{Abstract}

Malicious use of automated bots presents an increasing risk to applications on the internet. Their goals range from information gathering to denial of service attacks to fraud and many more. Bot detection represents an integral part of modern security systems. Existing solutions do either not perform well, are not accessible to many providers due to high cost, or disregard modern privacy  standards. This work aims to provide a proof-of-concept for a basic system that incorporates all of the above criterions. It utilizes request metadata and biometric data in the form of pointer interactions. Compared to previous works this thesis focuses on real world applicability in both data collection and evaluation by additionally simplifying the machine learning model and collecting a dataset of matching, and thus better comparable, request and mouse data. The proposed system achieves 96.6\% accuracy when tested with this work's dataset and up to 99.7\% when incorporating an external dataset. With some compromises in performance, it is also tunable for high execution speed.

\tableofcontents

\chapter{Introduction}

The ability to identify the current partner in a communication system as a human or computer is desirable in many contexts. Bots can be used to achieve malicious goals which operators want to prohibit and can be grouped into the following categories. DoS attacks aim to overload the target's infrastructure such that it becomes inaccessible for normal use. Carding and Credential stuffing refers to performing payment or login requests to find working credit card numbers and credentials which are, for instance, obtainend from a data breach. Data scrapers download website information and can use it for malicious purposes, e.g. damage SEO or violate copyrights. Content spam includes inserting malicious or polluting data on platforms that allow user generated content. Scalping or inventory hoarding of shopping items can artificially raise prices, damage brands, generate false market forces and create a bad customer experience. Imperva's 2022 Report\cite{BAD_BOT_REPORT2022} cites Account Takeover, Scraping and Scalping as the top three most common attack types.

In this work, the term bot is referring to software that automatically performs HTTP(S) requests with the intent of harming a target or reaching another malicous goal. While this threat is nothing new to the web the attack surface has grown significantly over the past years. In their yearly report, the Imperva Threat Research Lab\footnote{\url{https://www.imperva.com/blog/}} shows that both the percentage of bot traffic in general and the absolute amount of malicious bot traffic have increased over time.\cite{BAD_BOT_REPORT2020,BAD_BOT_REPORT2021,BAD_BOT_REPORT2022} For example 27.7\% of 2021's internet traffic was fraudulent and automatically generate compared to 25.6\% in 2020.

Most of the above attacks need to trick a webserver and their application backends into performing the request as if it had been initiated by a human. Instead of combating the resulting issues separately, bot detection could potentially mitigate many at once.

A complication in this problem space is the, often desired, requirement for non-malicious bots to be granted normal access. A prominent example are scraper bots used by search engines that need to request websites periodically to build their search indices. A common technique to exploit this requirement is trying to emulate known bot signatures from large search engines, e.g. Googlebot \cite{8421894}. According to Imperva's 2022 report \cite{BAD_BOT_REPORT2022}, these "good" bots made up 14.6\% of 2021's internet traffic.

Many website operators tend to use solutions that are easy to integrate. This requires embeding external software which collects user data and sends it to servers of the software vendors. Closed source software does not allow to determine what exactly happens to the user data and website operators open themselves to additional threats in case of a data breach. Depending on the operating countries of both the websites and software vendors, data privacy regulation might also not allow sharing user data at all or require the operator to document the data transfers in a very detailed and legally complicated way. For example, in countries falling under the GDPR \cite{GDPR} a comprehensive data protection documentation is required. Because of the above reasons it is desirable to either employ self-hosted software or use a solution that does not require user data transfers.

This thesis primarily focuses on detecting web-based bots that try to access or perform actions on websites which are the primary medium with which people interact on the internet. In contrast to related works its focus is set on the applicability in real world use cases and on trying to simplify the overall system to increase the ease of use and execution speed. Additionally a dataset of matching request and mouse data is collected to compare both data types as best as possible.

This work's main contributions are:

\begin{itemize}
    \item Validating previous works' results for machine learning based bot detection methods
    \item Collecting a dataset from real world websites and users with matching request and mouse data that can be directly compared
    \item Evaluating the proposed architecture's performance in scenarios which a typical deployment might encounter
\end{itemize}

This thesis is structured as follows. Chapter \ref{sec:requirements_and_related_work} presents the requirements for the bot detection system to be developed. It also outlines the progress previous works have already made in this context and describes its limitations. Chapter \ref{sec:method_concept} details the main concept and methods that are used. The user experiment, dataset acquisition, and bot data generation are introduced. The proposed machine learning systems and their different methods of feature extraction and preprocessing are described. In Chapter \ref{sec:eval} applicable research requestions are posed and their evaluation is performed. The results are shown, analyzed, and discussed afterwards. Finally Chapter \ref{sec:conclusion} summarizes the results and gives an outlook into possible future research options.


\label{sec:requirements_and_related_work}
\chapter{Requirements and Related Work}

Depending on several factors a bot detection system can have many different requirements. These include for example:

\begin{itemize}
    \item The types of bot that are expected to be used and need to be detected
    \item A focus on reliability and low false positive rate because disrupting the normal user flow for humans might be even less desirable than leaving some bots undetected
    \item The need for immediate decisions because following actions depend on the actor being a bot or human, e.g. CAPTCHAs
\end{itemize}

This chapter outlines the concrete requirements for this work's goals and outlines previously published works on these topics.


\section{Requirements}

The main requirement of the proposed bot detection system is its performance including being reliable, having a low false positive rate and good execution speed. In concrete terms, reliable, in this context, means a generally high accuracy. A low false positive rate is explicitely weighed higher than other metrics because accusing a human of being a bot and disrupting their usage of the application is considered worse than vice versa, where a bot might be falsely detected as a human. If the needed analysis cannot be run synchronously while the actor is performing actions, bots might be detected too late or some actions would need to wait for the detection system if they require a definite classification. This is why the system's execution speed is the next most important requirement.

If these factors are degraded by including other features, this work defines the overall value still lower as without. This work aims to verify previous works' results where mouse dynamics as one example of biometric data was used successfully to identify bots. Its performance is compared to a similar detection system which operates on request metadata only, representative of a non-biometric approach. Finally one goal is to reduce the system's complexity, increase its performance and test its applicability in a realistic environment.

Other challenges considered are ease of use for the operator and the possibility to be integrated into existing solutions. These goals are being made easier because, in contrast to proprietary solution, its implementation is explained in detail and available publicly.

The most desirable additional requirement is privacy friendliness which a machine learning system, that can be deployed within an operator's infrastructure, provides. Its parameters represent sufficiently anonymized user data that it can be more easily used and shared.

\section{Related Work}

The paper of Li et al. \cite{LiJi2021} introduces a federated learning approach similar to the goals of this work but differs in the specific use case and implementation. Their system focuses on the detection of IoT (Internet of Things) devices which are easily hacked and turned into zombies. These zombies are commonly used in DDoS (Distributed Denial of Service) attacks which their strategy tries to make not feasible to perform. They also develop their own iterative model averaging based method "gated recurrent unit" (GRU) which is optimized for their specific use case.

Iliou et al. \cite{10.1145/3339252.3339267} present a comparison of different machine learning algorithms and combinations of different attributes used in previous literature. Their methods were tested on a year's worth of HTTP log data from MK-Lab's public web server\footnote{Multimedia Knowledge and Social Media Analytics Laboratory, \url{https://mklab.iti.gr/}}. The data included IP addresses, request method, request path, referrers, user agent strings and timestamps. The attributes comprise mostly of request metadata which would be suitible for a privacy-friendly bot detection system, for example the percentage of image requests or the number of total bytes per session. The authors split the bot data in their dataset into simple and advanced bots which is determined by whether the requests have a browser agent name and, in case they do, whether the IPs have shown malicious activity before. Their results show that different sets of attributes are performing best depending on the classification algorithm used. The best machine learning methods are Random Forest and Multilayer Perceptron although the paper concludes that using an ensemble classifier that averages over all used methods would be more stable. Additionally simple web bots can be detected very easily while detecting advanced bots is significantly harder, with areas under the ROC curve of $1.00$ and $0.64$ respectively. Especially in false positive intolerant use cases the performance of detecting advanced bots is too poor to be used in the real world. The authors conclude that future work would need to incorporate more advanced features which can not be easily simulated by bots.


The work of Papadopoulos and collaborators \cite{PETS2021} outlines problems and privacy-related concerns in the context of bot detection and aims to solve a very similar problem but focuses on mobile devices. They explicitely comment on and compare their work against Google's reCAPTCHA v3 \cite{RECPATCHA_V3} which is a very prevelant but opaque solution. Especially the transmission of raw user data to third party servers (which often are located in a differen country) is considered pervasive and raises privacy concerns. In their own method "zkSENSE" the authors run a pre-trained machine learning model on the users' devices such that no user data has to be sent to their or any other servers. To avoid local changes to the model a cryptographic zero-knowledge proof is generated that is verified on a server. The model iself operates on data from the mobile device's sensors including accelerometers and gyroscopes. They postulate that a human-performed touch event or click generates specific information in these sensors that is either hard to fake or absent. In a comparison of different classifier models most perform very similar with Decision Trees and Random Forest being slightly better. Finally, they also tested the performance in terms of execution time, CPU and memory utilization, and consumed bandwith with the result that their method is comparable to existing CAPTCHA solutions.


The work of Shen et al. \cite{6263955} show the viability of using mouse and trackpad actions to verify the authenticity of users. Shen et al. use a pattern-growth-based mining method in which they group raw mouse events into mouse actions, such as single click, double click or drag-and-drop. This data is combined with additional information of the current application type (e.g. surfing the internet, word processing or playing games), the screen area where the mouse movement occurred, window position and timestamp into a database of mouse operation sequences. They employ a pattern growth approach which identifies relationships within the data. The authors further state that the mined patterns cannot be used directly by a classifier and are thus converting into a feature vector containing data such as the time it took a user to click a button, movement speed or acceleration. Finally 3 different one-class detectors (Nearest Neighbor, single-layer Neural Network, Support Vector Machine) are compared resulting in the SVM method performing the best with false positive and false negative rates of 0.37\% and 1.12\% respectively. This was only achieved by incorporating 3000 operations and 30 minutes of processing for successful authentication. With a more practically feasible authentication time of one minute, the values for FPR and FNR increate to 44.65\% and 34.78\%. This drastically limits the applicability of this approach which the authors conclude as well.

Antal et al.\cite{9111596} and their previous paper \cite{https://doi.org/10.1049/iet-bmt.2018.5126} cover intrusion detection and user verification using mouse data. Their first paper analyses the Balabit dataset \cite{BALABIT_CHALLENGE}, and determines parameters for a machine learning system. The second publication also compares different datasets and explicitely mentions the bad reproducibility of previous work. The authors focussed on describing their experiments with great attention to this goal. Their approaches segment raw mouse data into mouse actions such as mouse move (MM) or mouse move and a click (point and click, PC). Multiple results are averaged to increase detection performance. Their intrusion detection system's evaluation resulted in an average accuracy of 80.17\% and an AUC value of 0.8761 when only using Balabit's training data and worse values of 72.29\% accuracy and 0.7746 AUC when additionally incorporating Balabit's test data. Their conclusion names the test data's short session length as the primary reason for the worse performance. Privacy concerns were cited as one the primary challenges of using such a methods in practice. But their good perfomance makes using mouse data a promising method if these concerns could be remedied.

Acien et al. \cite{Acien2020BeCAPTCHAMouseSM} show the feasibility of using biometric features for bot detection. They use both function-based and GAN-based mouse trajectory synthesis methods to generate training and evaluation data. Six different classifier types (Support Vector Machine, K-Nearest Neighbor, Random Forest, Multi-Layer Perceptron and 2 Recurrent Neural Networks with Long Short-Term Memory and Gated Recurrent Units respectively) are compared to each other with Random Forest performing the best. Combined their method can distinguish between humans and bots with up to 98.7\% accuracy with only one mouse trajectory as input. They conclude that, compared to state-of-the-art works, the usage of mouse data has unexploited potential in the context of bot detection. Their "BeCAPTCHA-Mouse Benchmark" data and implementations are unfortunately not available publicly.

In their 2004 paper, Pusara et al. \cite{10.1145/1029208.1029210} use mouse movement data to re-authenticate already logged in users with the goal of identifying anomalous behavior to combat insider attacks where a potential assailant might have access to an existing user session. Similar to Antal et al.'s \cite{https://doi.org/10.1049/iet-bmt.2018.5126} extracted features, the authors use both the raw mouse events and derivative data, such as angle speed, distance as well as values for the mean, standard deviation and third moment over multiple samples. Decision tree classifiers and a smoothing filter were used to reach average false positive and false negative rates of 0.43\% and 1.75\%, although with high variability depending on which concrete users were used as input. While these initial results are promising, the authors conclude that scalability might become an issue as their model needs to be trained for each combination of users. As this approach requires already logged in users and is designed to identitfy specific users only its application in the context of bot detection is limited but the successful usage of mouse data is shown.

% file:///Z:/uni/thesis/papers/mouse_feature_papers/A_New_Biometric_Technology_Based_on_Mouse_Dynamics.pdf
Ahmed et al. pursue the very similar goal of identifying users based on mouse dynamics. In an own experiment they collected 998 session of 22 participants
A MLP-based neural network with a single hidden layer of 40 neurons was used for classification. The raw data was segmented into the mouse actions types mouse-move, drag-and-drop, point-and-click and silence. Together with the corresponding distance in pixels, time, speed and direction (8 directions spanning $45\deg$) this data was passed through a noise reduction stage, followed by inputting it into the neural network. The authors conclude that, despite their results' good false positive and false negative rates of 2.4649\% and 2.4614\%, practically viable access control was not achieved. Despite this the values were still better than some established methods based on biometric data, e.g. voice or face recognition.

\section{Summary}

Due to the sheer size of the problem space in bot detection and human authentication scientific works need to limit their scope to generate any meaningful results. These limitations could be in the form of specific input data, methods or goals which result in compromises needing to being made in other parts. For example Iliou et al. \cite{10.1145/3339252.3339267} compare in great detail how different machine learning algorithms, their parameters and different input features perform on real world request metadata. But their poor performance for detecting advanced bots show that their findings might not be relevant when detecting such bots is desirable. Papadopoulos et. al \cite{PETS2021} limit their method to specific cases where mobile devices and data from their accelerometers and gyroscopes are used. This would not be applicable as soon as this data is not made available by the user or other devices are used. To bring the field of bot detection forward many different approaches need to be combined and also tested under real world conditions which this work aims to add to.


\label{sec:method_concept}
\chapter{Request Metadata and Mouse Dynamics for Bot Detection}

The thesis explores how biometric data can potentially improve a machine learning system for bot detection. Compared to data that is only indirectly generated by a human, for example web requests metadata, biometric data contains more information that can be directly used to decide whether it originated from a bot or human. A big part of request data is redundant as it either does not change between different requests or clients, e.g. the "User-Agent" header, or can be easily faked. Illustrated by Figure \ref{fig:user_mouse_heatmap}, human-generated mouse data contains many different movements which are hard to model mathematically and thus difficult for an attacker to imitate.

\begin{figure}[H]
    \includegraphics[width=\textwidth]{figures/user_mouse_heatmap.png}
    \caption{29 Mouse movements captured from a human user}
    \label{fig:user_mouse_heatmap}
\end{figure}

For classification a system based on machine learning is primarily chosen because it performed well in previous works, such as the papers from Iliou et al. \cite{10.1145/3339252.3339267} and Acien et al. \cite{Acien2020BeCAPTCHAMouseSM}. As they and other related works showed, machine learning systems have the potential to perform well enough for practical use but lacked either performance or execution speed, especially when trying to differenciate between different human users. When reducing the goal to only detecting bots versus any human, this work aims to show the increased viability. Its approach differs from similar works, such as Acien et al.'s BeCAPTCHA method \cite{Acien2020BeCAPTCHAMouseSM} in that it also hypothesises that a simplified approach in terms of model parameters, input feature complexity can be found to improve the practical feasibility, ease of deployment and execution speed. This is achieved by performing both the experiments and data collection as well as the evaluation in an environment which is representative of a real application.

A similar system to the proposed architecture of Iliou et al. \cite{10.1145/3339252.3339267} is implemented for detecting bots via request data. Their work is used as the basis for the machine learning system to compare against. They evaluated the most promising features that have been proposed over 5 years prior to their publication (2019) and accumulated their findings in concise results.

Mouse, touchpad or touch gestures (pointer data) are used as the second type of input data in this work's approach. This data is harder to fake by attackers trying to emulate human behavior. Because it would be classified as sensitive user data, this type of data collection and processing could not reasonably be used by a third party provider or without a strictly necessary reason and would require explicit user consent \cite{GDPR}. Thus it would be best to process the data in an anonymized and indirect way. Machine learning architectures allow learned knowledge about different bot behavior to be more easily transfered between systems in this indirect form without compromising user privacy in comparison to other methods. For example federated learning \cite{DBLP:journals/corr/KonecnyMR15} \cite{DBLP:journals/corr/KonecnyMRR16} is an approach that only shares internal model parameters which are sufficiently anonymized so that specific users cannot be identified. It consequently allows the incorporation of very sensitive personal data which, this work hypothesizes, will improve the performance. Compared to the often very artificially constructed environments of related works, this thesis tries to use a very realistic and web-based approach.

It is also shown that the requirement of good execution speed is met by integrating the trained model into the experiments' websites as representative real-world applications. As the recorded data is aggregated over time per user, each server response includes the current score as a number between 0 and 1. How much of the past data can be used without compromising request performance is additionally determined.

\section{Datasets}

To realistically match request metadata and user mouse data a dataset is gathered from two websites in a user experiment. This way biases of different datasets are eliminated and the different systems' performances can be directly compared against.

The websites have a basic blog-style structure, shown in Figures \ref{screenshot:fmexp1} and \ref{screenshot:fmexp2}. They include different information sections and user registration features. Their web servers log all relevant data to extract the features required for the request data approach while a javascript library records mouse data and sends it to the site's backends for storage.

While no suitible datasets exist that include both request and mouse data, datasets that contain real-world user mouse movement and interaction data are publicly available. For example the Balabit Mouse Dynamics Challenge dataset \cite{BALABIT_CHALLENGE} includes a few longer and several shorter sessions which are meant to be used for training and testing respectively.
Antal et. al's dataset \cite{9111596} includes 21 users with around 20-30 sessions each. Their work compares their own dataset against the Balabit and ChaoShen datasets. In their scenario of detecting whether mouse data belongs to a specific user, the former corresponds to better but comparable results.
At the time of publication the referenced ChaoShen dataset is not available anymore. Antal et. al's dataset is used to validate the transferability of this work's method because of its better performance and because the other datasets are older and have been used more in other publications.


\section{Machine Learning Model}

Many different machine learning models are suitable for binary classification. Hu et al. \cite{8275816} compare different classifiers in a context where mouse data is used. Random Forest and Multilayer Perceptron (MLP) perform the best.
Iliou et al. \cite{10.1145/3339252.3339267} also show that Random Forest and MLP perform well when using request metadata.

Multilayer Perceptron describes a type of feedforward artificial neural network which connects many perceptron algorithm instances. It is inspired by biological neural networks and has at least three layers (input, hidden, output) which are usually fully connected. By iterating over training data, values for the perceptron's weighted inputs are learned such that the overall model behaves as a classifier or system for regression analysis. The learning itself is accomplished by using error functions which provide feedback on the results, e.g. in the form of a gradient which is used to change the weights in certain directions.

Random Forest\cite{Breiman2001} is a perturb-and-combine technique that uses an ensemble of decision trees. Decision tree learning in general is a supervised machine learning method which can be used for classification or regression depending on the output being a discrete value (i.e. class) or real number. For each node of the tree a binary decision criterion is calculated and the data is split accordingly. With increasing tree depth this approach is able to approximate very irregular models but also has a high risk of overfitting. Random Forests extend this technique by randomizing the input sample selection and parameters with the goal of decreasing the variance and avoiding overfitting. Choosing a random subset of inputs repeatedly is generally known as bootstrap  aggregation or bagging. The Random Forest algorithm in particular additionally randomizes the input selection at each node when growing the tree to further increase the resulting benefits. The final prediction is calculated from the average over all trees. The used implementation's documentation notes that compared to the original publication, their method averages over the probabilistic predictions, instead of only single decisions.\footnote{\url{https://scikit-learn.org/stable/modules/ensemble.html\#random-forests}} Their decision trees' used split criterion function is the Gini impurity which compares the amount of incorrect labeling of randomly chosen samples if their label (for the current feature) was also chosen randomly to the currently assigned label.

As their performance is comparable in this context and Random Forest's implementation is easier as well as more performant compared to MLP, the former is chosen for this work.

\subsection{Model Parameters}

The most important parameters of the Random Forest algorithm are the number of estimators (i.e. number of decision trees) and the maximum number of randomly selected features per estimator. In general as more trees are used less overfitting occurs and the model generalizes better. Additional parameters include the minimum number (or fraction) of samples required to split a node (\lstinline{min_samples_split}). When using the minimum value of two, node splitting is effectively unrestricted. Bigger values also affect the maximum tree depth. It can either be set directly as a parameter or is the depth at which all leaves contain no more than \lstinline{min_samples_split} samples. In the unrestricted case, this means that all leaves need to be pure when the maximum depth is reached. Additionally weights can be assigned to different classes or samples. For binary classification class weights are not considered and sample weights might be useful if there is additional data available before classification. When combining this type of machine learning system with other bot detection systems, their existing prediction could be added as weights. As this thesis only considers the classifier itself, no weights are used. Finally the maxmimum number of features that are tested when determining the node split can be set. Scikit-learn's implementation \footnote{\url{https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html}} uses the square root of the total feature count which is also recommended by previous literature. \cite{Hastie2009}

An empirical search is used to determine the best parameters for this use case. Additionally it is evaluated how the predictive performance changes for different amounts of training data.

\subsection{Feature Selection}

The following sections describe for both request and mouse data which features are chosen and how they are used as inputs for the Random Forest machine learning system. Previous works are referenced which already compared several different possiblities in similar contexts.

\label{concept_request_data}
\subsubsection{Request Data}

Iliou et al. \cite{10.1145/3339252.3339267} ranked the best performing metrics for simple and advanced bots per classification algorithm. For Random Forest, some attributes are not suitible in this theses. For example the authors include a boolean indicating whether a request has a known search engine in their "Referer" header (attribute 14). Because participants are asked to visit the websites directly this attribute is omitted. The websites are otherwise designed such that all of the above attributes are meaningful. The used attributes are listed in the implementation section.

The following selection from Iliou et al.' work \cite{10.1145/3339252.3339267} is used.

\begin{enumerate}
    \item The percentage of HTTP requests that led to an HTTP 4xx code response. (7)
    \item The percentage of HTTP requests that requested a css file. (10)
    \item The percentage of HTTP requests that requested a JavaScript file. (11)
    \item The percentage of HTTP requested URLs that contain the previously requested URL as a subpart. (20)
    \item The total time (in seconds) between the first and the last HTTP request of the session. (21)
    \item Standard deviation of requested pages' depth (i.e. number of "/" in URL path). (16)
    \item Mean and Standard Deviation of times between successive requests. (23)
\end{enumerate}

\label{concept_mouse_data}
\subsubsection{Mouse Data}

The mouse data features consist of the relative $x$- and $y$-coordinates as well as a time value for each mouse event. Additional features are engineered similar to Gamboa et.al.\cite{GAMBOA2004} and \cite{https://doi.org/10.1049/iet-bmt.2018.5126}, including the path length from the origin, angle of the path tangent, horizontal, vertical and overall velocity, acceleration, jerk, angular velocity. Additionally the type of action, length of the movement and time needed to complete the action will be used. Some features that are considered in the authors' work have been omitted with the goal of simplifying and speeding up the feature extraction which can be revisited if the performance seems poor.

Single mouse data points are grouped based on the following rules: They either end with a click, have a maximum of 50 data points or span a maximum of two seconds. The features are calculated for each group. As a basis for all derived values, the time, x and y coordinates are linearly interpolated such that vectors with uniformly spaced values $x'_t$ and $y'_t$ every $20ms$ are generated. All indices start at zero.

The path length from the origin $s_t'$ is the accumulated sum of previous segment lengths:

\[
s'_t = \sum_{k = 0}^{t - 1} \sqrt{(x'_{k+1} - x'_{k})^2 + (y'_{k+1} - y'_{k})^2}
\]

The angle of the path tangent with the x-axis $\theta_t$ is the arctangent ($atan2$ is used, which returns only values $-\pi < \theta < \pi$) of the segment at time $t > 0$. At $t=0$ an angle of $0$ is assumed.

\[
\theta_t = atan2( (y'_{t+1} - y'_{t}), (x'_{t+1} - x'_{t}) )
\]

The temporal features horizontal ($v_x$), vertical ($v_y$), tangential ($v$) and angular velocity ($\omega$) as well as tangential acceleration ($\dot{v}$) and jerk ($\ddot{v}$) are computed as follows:

\[
v_x = \frac{\delta x}{\delta t}; \quad
v_y = \frac{\delta y}{\delta t}; \quad
v = \sqrt{v_x^2 + v_y^2}; \quad
\omega = \frac{\delta \theta}{\delta t}; \quad
\dot{v} = \frac{\delta v}{\delta t}; \quad
\ddot{v} = \frac{\delta \dot{v}}{\delta t}
\]

For each of the $9$ vectors $(x'_t, y'_t, s'_t, v_x, v_y, v, \omega, \dot{v}, \ddot{v})$ the mean, standard deviation, minimum, maximum and value range (max - min) yield the first $45$ feature values.

Additionally the time $t_{total}$ and length $s_{n-1}$ of the stroke (i.e. group of $n$ data points), its straightness and jitter are computed. The time is the difference between the first and last datapoints' timestamps and the length can is the accumulated sum of segment lengths but using the raw instead of the interpolated data.

\[
t_{total} = t_{n-1} - t_0
\]

\[
s_{n-1} = \sum_{k = 0}^{n - 1} \sqrt{(x'_{k+1} - x'_{k})^2 + (y'_{k+1} - y'_{k})^2}
\]

Analogous to Gamboa et.al.'s definition\cite{GAMBOA2004} the straightness is defined as the ratio of the Euclidian distance between the first and last points of each group, and the total distance:

\[
straightness = \frac{ \sqrt{ (x_0 - x_{n-1})^2 + (y_0 - y_{n-1})^2 } }{s_{n-1}}
\]

The jitter is the ratio between the original and smoothed path lengths:

\[
jitter = \frac{s'_{n'-1}}{s_{n-1}}
\]

In total the 49 values make up the input vector that is computed for each mouse action group. They are paired with the corresponding "is-bot" or "is-human" label depending on which user they originate from.


\subsection{Classification and Evaluation}

The actual classification is performed by splitting the extracted feature data points into training and test sets. The partitions contain 90 and 10 percent of the data, respectively, with each data entry being assigned randomly. The training data also includes the "is bot" label which the model uses to calculate its error while learning. In the testing phase a predicted label is compared to the known value. The fraction of correct over the total number of predictions, i.e. accuracy is used as the primary evaluation metric. Additionally the False Positive Rate (FPR) and False Negative Rate (FNR) are computed which are the fraction of wrong guesses. The False Positive Rate is weighed higher because a prediction that a user is a bot has a qualitatively much greater impact than the opposite case. Plotting one against the other results in a Receiver operating characteristic (ROC) curve. The area below this curve (AUC) is another metric used for comparing results because it provides a single and condensed numerical value. When using normalized units, its concrete value represents the probability that the model will perform better when given a randomly selected positive and negative sample.\cite{FAWCETT2006861} Depending on the context, other useful metrics are precision and recall. Precision is the ratio of the true positives and all positively classified results. Recall is the ratio of true positives and all actually positive data points. As a high precision value is directly correlated with a low amount of false positives, it is considered more important in this work's context.

\section{Experiment and Data Collection}

A user experiment was announced to all members of the University of Hamburg to gather a representative dataset for human interactions on websites. The specific goal of the experiment was not directly included in the announcement to avoid any initial biases. To gather all required data a custom website with two different frontend layouts and designs was implemented.

\label{concept_websites}
\subsection{Websites}

The structure and content mimic blog-style websites with login and register functionalities (see Figure \ref{screenshot:fmexp1_register}). They contain a landing page, an about page with additional information about the experiment and this thesis and a contact / imprint page. On first visit users get presented a dialog asking to confirm the data collection (see Figure \ref{screenshot:fmexp2_consent}). After registering or logging in a profile page is accessible where basic user information can be added or edited. The main part of both websites is a blog section containing randomly generated entries which can be viewed on an overview page or on detail pages which only show a single entry.
All parts give opportunity for user input, similar to regular websites including many possible navigation paths and form interactions.

One difficulty of recording both request and mouse data is that the former benefits from having as many requests per interaction as possible while full page reloads would restart the mouse data collection and might lose data. Usually websites would decide between a traditional approach, where URL changes would load a completely new page and fulfill the former requirement, and a SPA (single-page application) where only parts of the content are changed dynamically by JavaScript implementations which would allow mouse events to be recorded continuously.

To combine the advantages of both approaches a hybrid system is used that intercepts URL changes, e.g. from clicking a link, preventing the default action and loading the target page through a separate HTTP request. The main content section is then replaced by the newly loaded HTML data. From the backend's point of view at least one request is performed per visited page while mouse data is recorded without any gaps.

All users, logged in or not, are identified by a version 4 UUID which is immediately stored in cookie after accepting the initial dialog prompt. Every data point that is written to the database gets a reference to the correspoding user and the current date and time.

\subsection{Request Metadata}

For the request metadata relevant information of every request is recorded. Flask allows to register functions which are called at specific points in the request lifecycle. For this purpose the \lstinline{after_request} decorator is used which is called right before the handled request is returned.

Bot data is generated by sending requests to the same websites but including an additional query string which details the current bot configuration.

The following data is stored in the database:

\renewcommand{\arraystretch}{1.2}

\begin{table}[H]
\begin{center}
\begin{tabular}{ll}
\toprule
Name & Description \\
\midrule
user uuid & The current user's UUIDv4 \\
request method & One of \{'GET', 'POST'\}, other request methods are not used \\
request url & The full request URL including \\ & scheme, host, root path, path and query string \\
request path & The request path \\
request origin & The host from which the request was sent from \\
request remote address & The IP address of the client \\
request referrer & The referer value from the corresponding header field \\
response content type & The response's body media type \\
response content length & The response's body size in bytes \\
response date & The date and time of the response \\
response status code & The HTTP status code of the response \\
user type & One of {Bot, Human} \\
random delays & Whether random delays for bot-generated requests were enabled \\
advanced & Whether the advanced bot version was used \\
bot mode & One of \{Request, Mouse\} \\
data type & One of \{Request, Mouse\} \\ & saved separately because e.g. mouse bots also generate request data \\
\bottomrule

\end{tabular}
\end{center}
\end{table}

\subsection{Mouse Data}

All information for the mouse datasets is recorded by a JavaScript library which is deployed as part of a VueJS application. It registers listener functions for the events \lstinline{pointermove, pointerdown, pointerup} on the whole document which are throttled to $30$ events per second. Pointer events are used because they include mouse, touch, pen/stylus and other pointing devices. For every event the following data is recorded:

\begin{table}[H]
\begin{center}
\begin{tabular*}{\textwidth}{ll @{\extracolsep{\fill}}}
\toprule
Name & Description \\
\midrule
user uuid & The current user's UUIDv4 \\
type & One of \{'pointermove', 'pointerdown', 'pointerup'\} \\
position & The pointer's x and y screen coordinate in pixels \\
document size & The document's width and height in pixels \\
pointer type & One of \{'mouse', 'pen', 'touch'\} \\
buttons & What buttons are pressed as a binary encoded integer \\
dt & The date and time of when the event occurred \\
\bottomrule
\end{tabular*}
\end{center}
\end{table}

All events are cached locally in a list which is sent to the backend every two seconds which writes every data point individually to the database. This request is excluded from the request data capture.

\section{Bot Data Generation}

To generate bot data the different variants were implemented by using the selenium-python and puppeteer (JavaScript) libraries.\footnote{\url{https://selenium-python.readthedocs.io/}} \footnote{\url{https://pptr.dev/}} They are designed to run automated actions in a browser environment.

Most bot variants were implemented as a generic class \lstinline{Bot} which sets up a selenium webdriver instance and provides wrapper methods for randomly waiting between calls and locating elements. When instantiated it starts a  browser instance with a specific window size. Firefox was used as the browser backend and 1366x768, 1920x1080 and 2560x1440 were used as the window sizes as these represent very common monitor resolutions.\footnote{\url{https://gs.statcounter.com/screen-resolution-stats/desktop/worldwide}} The class also handles the initially required request to set the current bot parameters and saves the returned cookie. The advanced mouse bot was implemented separately with puppeteer and the extension ghost-cursor \footnote{\url{https://github.com/Xetera/ghost-cursor}} because many instances could be run in parallel. After initially planning to use the PyAutoGUI \footnote{\url{https://pyautogui.readthedocs.io/en/latest/}} library, it turned out that its method of faking a cursor was not compatible with running headless browser instances without GUIs. The puppeteer automation was otherwise implemented to have the exact same methods as the selenium variant.

The classes \lstinline{RequestBot} and \lstinline{MouseBot} inherit from \lstinline{Bot} and implement methods for the following actions:

\begin{enumerate}
    \item Accepting the initial prompt dialog to start the experiment
    \item Visiting the top-level pages \{About, Blog, Contact/Imprint, Login, Register\}
    \item Visiting 10 randomly selected single blog pages with a parameter for how many
    \item Visiting 100 completely randomly selected pages with a parameter for how many
    \item Registering an account
    % \item Filling in profile data
\end{enumerate}

Randomly choosing the next link to click in action 3. and 4. increases the data's variance greatly by allowing for many different combinations of start and end points for mouse movements.

All actions are configued to wait for the target element to be visible and clickable, scrolling it into view, if not. If configured as such, before each action a random delay between $0$ and $2$ seconds is waited for.

The request bot primarily uses selenium's basic \lstinline{click}, \lstinline{back} and \lstinline{send_keys} methods to perform the actions. Barring the random delays it will perform the actions as fast as possible.

% \subsection{Mouse Bot}

The mouse bot uses the same Selenium-based methods for locating elements but replaces the basic actions with Selenium's ActionChains which can automate low-level interactions including mouse movements, mouse button actions and keypresses. By default it interpolates linearly between the starting and ending locations of the movement. Ghost-cursor's implementation uses bezier curves with randomly selected control points in a limited area on one side of the line between start and end points. Additionally the specific move and click coordinates are also selected randomly from the target element's area. Figure \ref{fig:bot_mouse_heatmap} visualizes the generated mouse movements from one bot session.


\begin{figure}[h]
    \includegraphics[width=\textwidth]{figures/bot_mouse_heatmap.png}
    \caption{Advanced Bot Mouse Movements Example}
    \label{fig:bot_mouse_heatmap}
\end{figure}



\section{Implementation}

The basis of the experiment's websites is a Flask\footnote{\url{https://flask.palletsprojects.com/}}-based python application. They are deployed behind a nginx reverse proxy web server which exposes them on two public domains. All web and application servers run on a bare-metal linux computer located in a German datacenter and are configured as real applications would be in a production environment. A postgresql server with two separate databases is also deployed. For evaluation their data was mirrored to local postgresql instances and queried directly from the same python application that runs the websites to avoid code duplication and benefit from SQLs expressiveness.

In order to extract the feature data the stored unique users are filtered based on a, to be determined, minimum number of data points. In a multi-threaded pre-processing step the feature data is queried per user and labeled as "human" or "bot". The request data is aggregated per user and the features are calculated as described in the \nameref{concept_request_data} section.

The mouse data is also calculated per user but grouped into actions. Each action represents a mouse movement that is either a maximum of $2$ seconds long or ends with a click. It is also ensured that within any action the screen resolution or page scroll position does not change.

For training and classification the implementation uses the RandomForestClassifier class from the scikit-learn\footnote{\url{https://scikit-learn.org/stable/}} library. The standard methods \lstinline{fit}, \lstinline{score} and \lstinline{predict} are used to train the model and test its accuracy. Additional metrics are calculated using \lstinline{predict_proba} to get the probability per input data point and custom methods. Classes and launch scripts are implemented for bot data generation, loading the external dataset and generating figures for visualization.


\section{Summary}

This work's concept explores why a system base on machine learning is best suited for the stated problem of bot detection. It proposes a simplified model to achieve better usability and execution speed compared to previous works. The proposed architecture shows how the required data can be collected from basic websites, representing a big range of possible applications. Insights from previous works are considered for what kind of data is best suited for the machine learning model which are subsequently defined. Finally the concrete implementation is briefly outlined.


\label{sec:eval}
\chapter{Evaluation}

To evaluate this work's results the following research questions are posed:

\begin{itemize}
    \item How do the simple and advanced methods of bot data generation compare?
    \item Does the trained machine learning model generalize to data from another website or external dataset?
    \item How does mouse dynamics data perform compared to request metadata?
    \item How is the architectures real world applicability in terms of execution speed and performance?

\end{itemize}

The evaluation starts by analyzing the user experiment and filtering out unusable data. After generating the synthetic bot data, it was determined what impact limited versus all available data has on the performance. Subsequently the best Random Forest model parameters were determined empirically. The model's performance was also measured in different scenarios and compared to the request metadata method. Then the basic and advanced mouse bot variance were compared. Multiple combinations of training and test data including an external dataset were evaluated to reason about the generalizability of both the experiment's data as well as this work's method in general. Finally its execution speed was analyzed to determine its applicability.

\section{Dataset}
% \section{Experiment Data Quality}

Before extracting the input features for the machine learning model, the website's collected data was analyzed. The experiment assumed that participants use the provided website frontends which set cookies to map recorded datapoints to specific users. The first website's basic statistics showed that 124867 unique users have been recorded which indicates that actual bots were used to visit this website which did not execute JavaScript and set the cookie correctly. The histogram of the number of datapoints per users confirms that the vast majority of users only map to zero to one datapoints. With the chosen threshold of two datapoints per user the experiment resulted in recording 322 and 163 participants on the two websites respectively. Figure \ref{fig:user_dp_hist} shows the distribution of the number of user (request and mouse) in terms of the number of datapoints.

\begin{figure}[h]
    \includegraphics[width=\textwidth]{figures/user_dp_hist.png}
    \caption{Distribution of users in terms of datapoint count}
    \label{fig:user_dp_hist}
\end{figure}


% \section{Limited Data}

With the goal of measuring limitations and the scalability in terms of the amount of data being available, the number of data points per user session were limited. In contrast to offline analysis, where limited data would not be an issue, the fewer data points are needed in (close to) realtime analysis for a satisfactory classification the better to detect bots as quickly as possible. Especially machine learning based systems scale generally better the more input data is available. For example Iliou et al.'s result of $1.00$ AUC and $0.01$ FPR in their simple bot evaluation \cite{10.1145/3339252.3339267}, which is best comparable to this work's generated bot data, shows that near perfect performance can be achieved. Conversely a balance needs to be found between amount of data and performance.

With less limitations the request data model's evaluation metrics show increasingly good performance. Table \ref{table:request_params_no_limit} shows the detailed results. There are however several problems with this seemingly good performance. As Figure \ref{fig:user_dp_hist} shows, there are many cases where less datapoints than the limit are available. This results in lower values for features which calculate an absolute metric instead of a percentage or relative one, e.g. feature four and five described in Section \ref{concept_request_data}. These differences in absolute values can be easily learned and overfitted to by the machine learning model but don't represent actual distinct bot or human behavior.
Another problem is the variability of available requests in a practical setting. As mentioned in Section \ref{concept_websites}, many websites are single-page applications where an initially loaded JavaScript bundle contains many website components such that less subsequent requests need to be made. Thus a worst-case scenario would be that only 2 requests are initially available. More commonly additional resources, such as images, css files or api data might be loaded but even in this case the initial requests would be the same between humans and bots as they are automatically performed by the browser and are independent of any interaction with the website itself. As a middle ground 10 data points are chosen for the remaining experiments. This amount of requests can be assumed to be available in almost any context.

Similarly the mouse data model performs generally better the more data is available. Table \ref{table:mouse_params_no_limit} lists the performance for different amounts of data points per user. A big advantage of this approach is that more data points can potentially be aquired in a shorter amount of time compared to request data. For example, when sampling at $30$ events per second, as this work's implementation does, it only takes $1.66s$ ($50$ samples) to capture the amount of data points needed to surpass the request data model's performance. When, for example, considering a potential application for a CAPTCHA, this time does not represent a significant disruption of most user interactions, e.g. filling in a registration form.

\begin{table}[H]
    \begin{center}
        \begin{tabular}{rrrrrr}
            \toprule
            Data Points / User & Accuracy & Precision & Recall & AUC & Training Time (s) \\
            \midrule
            200 & 0.980 & 0.980 & 0.980 & 0.982 & 0.209 \\
            100 & 0.970 & 0.961 & 0.980 & 0.985 & 0.209 \\
            No limit & 0.960 & 0.960 & 0.960 & 0.972 & 0.215 \\
            50 & 0.950 & 0.941 & 0.960 & 0.976 & 0.208 \\
            20 & 0.910 & 0.918 & 0.900 & 0.975 & 0.221 \\
            5 & 0.900 & 0.885 & 0.920 & 0.945 & 0.220 \\
            10 & 0.890 & 0.842 & 0.960 & 0.956 & 0.220 \\
            4 & 0.880 & 0.913 & 0.840 & 0.922 & 0.211 \\
            3 & 0.790 & 0.822 & 0.740 & 0.877 & 0.214 \\
            2 & 0.690 & 0.744 & 0.580 & 0.774 & 0.226 \\
            \bottomrule
        \end{tabular}
    \end{center}
    \caption{Request data model performance with varying amounts of data points per user}
    \label{table:request_params_no_limit}
\end{table}

\begin{table}[H]
    \begin{center}
        \begin{tabular}{rrrrrr}
            \toprule
            Data Points / User & Accuracy & Precision & Recall & AUC & Training Time (s) \\
            \midrule
            No limit & 0.966 & 0.964 & 0.968 & 0.993 & 0.449 \\
            50 & 0.933 & 0.943 & 0.922 & 0.979 & 0.124 \\
            200 & 0.930 & 0.951 & 0.906 & 0.979 & 0.189 \\
            100 & 0.920 & 0.942 & 0.895 & 0.975 & 0.149 \\
            20 & 0.855 & 0.846 & 0.868 & 0.949 & 0.114 \\
            10 & 0.850 & 0.862 & 0.833 & 0.903 & 0.120 \\
            5 & 0.846 & 0.909 & 0.769 & 0.916 & 0.099 \\
            4 & 0.826 & 0.895 & 0.739 & 0.879 & 0.098 \\
            \bottomrule
        \end{tabular}
    \end{center}
    \caption{Mouse data model performance with varying amounts of data points per user}
    \label{table:mouse_params_no_limit}
\end{table}

\section{Model Hyperparameters and General Performance}

To empirically determine the best Random Forest parameters both mouse and request data from the both experiment's websites and generated bots (same number of sessions as human users, advanced for mouse bot) were used to calculate the performance for combinations of the following parameters:

\begin{enumerate}
    \item Number of estimators (10, 50, 100, 150, 200, 1000)
    \item Maximum number of features (None, log2, sqrt)
    \item Maximum tree depth (None, 1, 2, 3, 4, 6, 7)
\end{enumerate}

Tables \ref{table:request_params} and \ref{table:mouse_params} show the $10$ best performing combinations for mouse  and request data. The data is sorted primarily by accuracy as the most important metric. The additional scores Precision, Recall, F1, AUC and the training time were computed as well. Their values of the top performing results lie close together except training time which is important in the context of applicability and is considered second most important. The different values for the number of estimators and the maximum number of features for split consideration perform very similarly. For request data, the chosen values for following experiments were chosen to be $100$ and $None$ respectively as their result had the same accuracy and AUC as the top result but its training time was significantly lower at $0.486s$. Analogously for the mouse data result, $200$ and $sqrt$ were chosen. All results have in common that no restriction to the decision trees' maximum depth is applied which is also the default value of scikit-learn's implementation. This is expected as the tree depth is directly correlated with the ability to classify multi-dimensional input data.

These results also give a first look into the expected performance with the model based on request data, in general, performing worse compared to mouse data in terms of accuracy. In addition to the increase in accuracy of $5.6$ percentage points, the latter model's precision and AUC values are better too. These are directly related to a lower false positive rate which is especially important in this bot detection context.

\begin{table}[H]
    \begin{center}
        \begin{tabular}{lrlrrrrr}
            \toprule Max Features & \# Estimators & Max Depth & Accuracy & Prec. & Recall & AUC & Tr. Time (s) \\
            \midrule
            None & 1000 & None & 0.910 & 0.887 & 0.940 & 0.973 & 4.348 \\
            log2 & 200 & None & 0.910 & 0.887 & 0.940 & 0.973 & 0.732 \\
            log2 & 1000 & None & 0.910 & 0.887 & 0.940 & 0.975 & 3.965 \\
            \rowcolor{green!30}
            None & 100 & None & 0.910 & 0.887 & 0.940 & 0.973 & 0.486 \\
            sqrt & 1000 & None & 0.910 & 0.887 & 0.940 & 0.972 & 3.912 \\
            log2 & 100 & None & 0.910 & 0.887 & 0.940 & 0.971 & 0.389 \\
            log2 & 150 & None & 0.910 & 0.887 & 0.940 & 0.972 & 0.694 \\
            None & 50 & None & 0.900 & 0.870 & 0.940 & 0.971 & 0.273 \\
            sqrt & 200 & None & 0.900 & 0.870 & 0.940 & 0.972 & 0.727 \\
            None & 150 & None & 0.900 & 0.870 & 0.940 & 0.972 & 0.703 \\
            \bottomrule
        \end{tabular}
    \end{center}
    \caption{Model accuracy for different parameters (request data)}
    \label{table:request_params}
\end{table}

\begin{table}[H]
    \begin{center}
        \begin{tabular}{lrlrrrrr}
            \toprule Max Features & \# Estimators & Max Depth & Accuracy & Prec. & Recall & AUC & Tr. Time (s) \\
            \midrule
            sqrt & 150 & None & 0.967 & 0.964 & 0.969 & 0.994 & 11.305 \\
            log2 & 1000 & None & 0.966 & 0.962 & 0.970 & 0.993 & 70.796 \\
            \rowcolor{green!30}
            sqrt & 200 & None & 0.966 & 0.962 & 0.970 & 0.994 & 5.899 \\
            sqrt & 50 & None & 0.966 & 0.964 & 0.968 & 0.993 & 9.280 \\
            sqrt & 100 & None & 0.966 & 0.962 & 0.969 & 0.994 & 9.815 \\
            sqrt & 1000 & None & 0.966 & 0.960 & 0.971 & 0.994 & 82.889 \\
            log2 & 200 & None & 0.964 & 0.963 & 0.965 & 0.993 & 20.381 \\
            log2 & 150 & None & 0.962 & 0.963 & 0.960 & 0.993 & 5.950 \\
            None & 150 & None & 0.961 & 0.954 & 0.969 & 0.991 & 91.586 \\
            None & 200 & None & 0.961 & 0.953 & 0.969 & 0.991 & 110.588 \\
            \bottomrule
        \end{tabular}
    \end{center}
    \caption{Model accuracy for different parameters (mouse data)}
    \label{table:mouse_params}
\end{table}

\section{Results}

The following scenarios aim to evaluate the method of bot detection using mouse dynamics in different realistic contexts and to answer the research questions. Initially it is determined whether an advanced method of generating fake mouse data can be detected better or worse compared to a simple approach. Data from both experiment's websites are then compared and used for training and testing separately. Next, the model's performance with an external dataset is evaluated. These results are compared to only using request metadata and additionally to a combination of both. Finally, the tradeoffs between execution speed and performance are determined.


\subsection{How Do the Simple and Advanced Methods of Bot Data Generation Compare?}

The first direct comparison used all available human mouse data and the generated simple and advanced mouse data respectively for both training and testing datasets. Table \ref{table:simple_vs_advanced_mouse} shows that the model performs better in every aspect and can be detected more reliably when using data generated by the simple bot. This is expected as the bots' linear movements are a uniquely identifying feature which result in very specific results for many input features. The model only correctly differenciated between a human and a bot $96.6\%$ of the time but still has a very high AUC. This results confusion matrix in Table \ref{table:mouse_advanced_confusion} and ROC curve in Figure \ref{fig:mouse_advanced_roc} illustrate the low FPR which is one of the primary requirements.

\begin{table}[H]
    \begin{center}
        \begin{tabular*}{\textwidth}{l @{\extracolsep{\fill}} rrrrr}
            \toprule
            Scenario & Accuracy & Precision & Recall & AUC & Training Time (s) \\
            \midrule
            Basic mouse bot & 0.995 & 0.997 & 0.994 & 1.000 & 0.857 \\
            Advanced mouse bot & 0.966 & 0.962 & 0.970 & 0.994 & 1.293 \\
            \bottomrule
        \end{tabular*}
    \end{center}
    \caption{Simple and Advanced Mouse Data Performance}
    \label{table:simple_vs_advanced_mouse}
\end{table}

\begin{table}[H]
    \begin{center}
        \begin{tabular*}{\textwidth}{l @{\extracolsep{\fill}} rr}
            \toprule
             & Positive prediction & Negative prediction \\
            \midrule
            Positive value & 947 & 29 \\
            Negative value & 37 & 939 \\
            \bottomrule
        \end{tabular*}
    \end{center}
    \caption{Simple and Advanced Mouse Data Confusion Matrix}
    \label{table:mouse_advanced_confusion}
\end{table}

\begin{figure}[H]
    \includegraphics[width=\textwidth]{figures/mouse_advanced_roc.png}
    \caption{Advanced mouse bot ROC curve}
    \label{fig:mouse_advanced_roc}
\end{figure}

\subsection{Does the Trained Machine Learning Model Generalize to Data From Another Website or External Dataset?}

The next experiment evaluates how well advanced bots can be detected when only training with basic mouse data and vice versa. All available human data was included in training. Table \ref{table:simple_vs_advanced_mouse_separate_train_test} shows that the model expectedly generalized poorly when training with basic bot data with an accuracy of only $52.29\%$ which is only slightly better than random guessing. Training with advanced bot data resulted in an increased accuracy of $76.9\%$ when trying to detect basic bots. The low recall value implies a high false negative rate and means that many bots remained undetected. These results can be explained by the different data generation methods.

\begin{table}[H]
    \begin{center}
        \begin{tabular*}{\textwidth}{l @{\extracolsep{\fill}} rrrrrr}
            \toprule
            Scenario & Accuracy & Precision & Recall & AUC & Training Time \\
            \midrule
            Adv. tr. data, basic test data & 0.769 & 0.938 & 0.577 & 0.928 & 1.306 \\
            Basic tr. data, adv. test data & 0.529 & 0.952 & 0.061 & 0.765 & 0.888 \\
            \bottomrule
        \end{tabular*}
    \end{center}
    \caption{Model performance with separate training and test data}
    \label{table:simple_vs_advanced_mouse_separate_train_test}
\end{table}

It follows the more relevant comparison of training with one website's data and testing with the other's in Table \ref{table:website_mouse_compare}.

\begin{table}[H]
    \begin{center}
        \begin{tabular*}{\textwidth}{l @{\extracolsep{\fill}} rrrrrr}
            \toprule
            Scenario & Accuracy & Precision & Recall & AUC & Training Time \\
            \midrule
            W1 training data, W2 test data & 0.966 & 0.955 & 0.964 & 0.993 & 1.075 \\
            W2 training data, W1 test data & 0.914 & 0.715 & 0.961 & 0.983 & 0.524 \\
            \bottomrule
        \end{tabular*}
    \end{center}
    \caption{Model performance comparing training/testing with different website's datas}
    \label{table:website_mouse_compare}
\end{table}

The first scenario has a $5.2\%$ higher accuracy of $96.6\%$ compared to training on the second website's data. This can be explained by the additional amount of user data collected from the first website. The second scenario's low precision also shows that not having enough data would compromise the required low False Positive Rate.


\subsubsection{Performance on an External Dataset}

To compare the performance to different real-world data, Antal et. al's dataset \cite{9111596} is used. Inputs from 21 users were collected during their normal computer activities on one desktop and 20 laptop devices. Both mice and touchpads were used. Their raw mouse movement and interaction data is preprocessed in a very similar way to the experiment's data. After conversion to the used model's format, the data is used as a large test set. For example "User1"'s 31 session files include $2.6M$ raw datapoints which are integrated into $64k$ input vectors. The whole dataset yielded $1.54M$ input vectors.

For comparison the model is trained on data from both websites and the advanced bot data. When testing the model against the external dataset, it achieves $82.25\%$ accuracy. As the test data is heavily unbalanced, some evaluation metrics need to be considered cautiously. For example, an AUC of $0.9976$ seems to indicate very good performance but does not give any insight into how well the dataset's humans were detected as such. With "is bot" being the positive class, the False Positive Rate of $22.13\%$ and Precision of only $52.89\%$ better show the actual performance. Reversing the setup and using the external data for training confirms that more data increases the model's performance. Its accuracy increases to $99.71\%$ and the values for FPR, FNR to $0.55\%$ and $0.15\%$ respectively. The comparison to the whole DFL dataset results in the model achieving an accuracy of $83.03\%$ with a reduced False Positive Rate of $17.16\%$. Although the data is even more unbalanced, the performance is slightly better.


\subsection{How does mouse dynamics data perform compared to request metadata?}

Confirming the hypothesis that a machine learning system using mouse dynamics outperforms one that uses request metdata, Tables \ref{table:request_params} and \ref{table:mouse_params} show the former's higher accuracy of 96.6\% compared to the latter's result of 91\%. Both methods have high AUC values of 0.994 and 0.973 respectively, indicating generally low negative rates. When evaluating these separately, the request data model has FPR and FNR values of 0.12 and 0.06 which means that it classifies more humans as bots compared to the opposite case. In a practical setting this would falsely identify roughly every eighth human user as a bot. In this regard, the mouse data model performs much better with FPR and FNR values of 0.0379 and 0.0297. Its false positive rate is still higher than the false negative rate but might, in absolute terms, be acceptable to be used in practice.

\subsubsection{Combined Mouse and Request Data}

In a real world application one might consider using both mouse and request data-based systems together in order to increase the overall performance. Because both data types from this work's user experiment can be matched and assigned to specific users, a direct comparison was conducted. The human and bot data was split at the user level into training and test sets to directly match each classifiers' result instead of at the input vector level. Because this resulted in relatively few test samples being available, an additional split ratio of 0.2 was used. For each user, both predictions are calculated as probabilities and averaged to determine the final result. Table \ref{table:combined_performance} shows that overall accuracies of 96\% and 95\% were calculated. This indicates that no significant improvement compared to using only mouse data was achieved. The most valueable difference to using the data separately is that no false positive results are produced. While the false negative rates of 0.272 and 0.327 are very high in contrast to the separate cases, the low false positive rate and overall same performance are in favor of using the combined approach.

\begin{table}[H]
    \begin{center}
        \begin{tabular*}{\textwidth}{rrrrrrrrr @{\extracolsep{\fill}}}
            \toprule
            Test ratio & Accuracy & Precision & Recall & AUC & TP & FP & TN & FN \\
            \midrule
            0.1 & 0.960 & 1.0 & 0.953 & 0.976 & 122 & 0 & 22 & 6 \\
            0.2 & 0.950 & 1.0 & 0.940 & 0.970 & 252 & 0 & 49 & 16 \\
            \bottomrule
        \end{tabular*}
    \end{center}
    \caption{Combined Mouse and Request Data Performance}
    \label{table:combined_performance}
\end{table}

\subsection{How is the architectures real world applicability in terms of execution speed and performance?}

The execution speed is measured by predicting a score after each request. This experiment only considers one user at a time which is why the actual accuracies differ from previous sections and are only meant to be compared in a relative manner. The human user having the most available datapoints and a correspondingly generated bot user is used. The total time, time without extracting the features and score are logged and averaged over multiple requests. These values are calculated for different amounts of previous data points which are incorporated. This is significant because the mouse features include many values which are accumulated over many data points. Figure \ref{fig:speed_per_dp_count} graphs the times and performance for the different numbers.

\begin{figure}[H]
    \includegraphics[width=\textwidth]{figures/speed_per_dp_count.png}
    \caption{Execution speed and score versus number of previous data points}
    \label{fig:speed_per_dp_count}
\end{figure}

The measured data shows that by only considering $100$ previous data points the total prediction time can be reduced to $27ms$ with a corresponding accuracy of $93.67\%$ compared to $900ms$ and $95.15\%$ ($2000$ previous data points). Reducing the amount of data points below $100$ yielded very low accuracies ($<80$) while not reducing the prediction time significantly. Compared to the average response times of all the experiments' requests of $23.78ms$ and $16.67ms$, this is still a very significant amount. One method which might be acceptable is to run the prediction in parallel to the request processing and waiting a few milliseconds for the result before returning a response. Another method would be to separate the feature extraction from the combination of data points. But for some values this might not be possible or lose information, e.g. mean or standard deiviation of several data.
A better method might be to separate the prediction system entirely from single requests and decide on a per-user basis asynchronously after enough samples have been collected whether to take any action against it or not.


\section{Discussion}

In the context of bot detection and the web in general, the methods of attack and type of a target(-website) can vary significantly and are changing constantly. This is one of the reasons that web security is an ongoing cat and mouse game and represents a practically unsolvable problem with many unkowns. This work needed to make many assumptions in order to limit the scope and produce generalizable data.

One limitation is that this approach does only work for clients that use a pointer device, i.e. mouse, touchpad or touch screen. If this is not acceptable, a different method or other biometric data would additionally need to be used. This applies for example when screen readers, text-based browsers or automated clients should be supported. A compromise might be to use the pointer-based system to trigger additional security checks at critical actions, such as registering an account or posting content.

Another problem might be that this approach does not elimitate all user tracking and the associated data processing. Especially public pages that should to be accessed without the need to ask for explicit confirmation. Although this is a general problem for websites that store or process user data, e.g. in the form of cookies, it applies to this approach as well. The main advantage is that the proposed system can be run by the same operator and does not need to send user data to any third parties.

Even the best-case accuracy of $>99\%$ might be not enough for some applications, especially if critical decisions depend on the system.

\section{Summary}

The Evaluation shows that the different machine learning systems perform very different depending on the choice of parameters and how much input data is available. Parameter values corresponding to the best results are determined and the tradeoff between available data amount and performance is outlined. The comparison of simple and advanced bot mouse data reveals that the simple method can be detected with almost perfect accuracy. The advanced method is able to fool the classifier, so that false values are generated.

The main takeaways from using the available experiment data and the external dataset in different combinations separately for training and testing are that the model's learned knowledge is transferable to different data in general. When training on limited data, the system did only achieve an accuracy of 82.25\%. Using the much larger external dataset for training demonstrates that the availability of more training data is very valuable. This case resulted in an accuracy of 99.71\%.

The question of how mouse dynamics performs compared to request metadata can be answered in that, overall, using mouse data results in better values of all relevant evaluation metrics. Combining both approaches does not increase the accuracy but results in lower false positive rates.

The real world applicability was evaluated in terms of execution speed and the performance tradeoffs. In general, as long as enough data can be captured, the system's viability to be used in practice is still given. If the prediction time needs to be reduced such that user interactions are not Interrupted significantly, e.g. $27ms$, the accuracy drops to $93.67\%$. This might be high enough for some applications or multiple results could be accumulated.

Lastly, shortcomings and limitations of the examined system are discussed. The biggest concern is that this approach would still only detect a subset of all possible bots, i.e. web based and using pointer or request data. Also, for applications where very high reliability and performance are required, the results are not sufficient to be practically used.

\label{sec:conclusion}
\chapter{Conclusion}

The Evaluation shows that in general a bot detection system which incorporates pointer data can be considered for practical use. Its main performance criteria in the form of high accuracy and a low false positive rate are given but its actual value is dependend on the concrete usage, operation and performance needs. The requirement to be privacy-friendly is met by using a system based on machine learning which can be deployed by operators themselves and does not need to share any identifiable user data with third parties. Finally, good execution speed is not achieved in every case and does degrade the overall value of the proposed system. In practice, higher accuracy can be traded for higher execution speed. Additionally the system has many possibilities to be optimized and improved which would meet this requirement as well. Ultimately the mouse dynamics approach cannot solve the problem of bot detection by itself and should not solely be relied upon but can definitely improve overall performance as part of a composite system.

Potential future works might want to consider more combinations of different approaches and their relative performance. Another direction would be an evaluation of whether training machine learning based bot detection systems directly in the browser is viable. This decentralized approach would result in lower computing cost for the operator and even better user privacy. More comparisons of state-of-the-art approaches in bot detection with different datasets would also bring the field forward. Additionally, better accessible and more public datasets for validation and training might be valuable.

\begin{raggedright}
  \printbibliography
\end{raggedright}


\appendix

\chapter{Appendix}

\section{Source Code}

\begin{itemize}
    \item Experiment Websites and Machine Learning Model: Github repository\footnote{\url{https://github.com/timesqueezer/fmexp}} and attached USB drive
    \item Thesis: Github repository\footnote{\url{https://github.com/timesqueezer/uni/thesis}}
\end{itemize}

\section{Screenshots}

\begin{figure}[H]
    \includegraphics[width=\textwidth]{screenshots/fmexp1.png}
    \caption{The user experiment's first website}
    \label{screenshot:fmexp1}
\end{figure}

\begin{figure}[H]
    \includegraphics[width=\textwidth]{screenshots/fmexp2.png}
    \caption{The user experiment's second website}
    \label{screenshot:fmexp2}
\end{figure}

\begin{figure}[H]
    \includegraphics[width=\textwidth]{screenshots/fmexp1_register.png}
    \caption{Registration section}
    \label{screenshot:fmexp1_register}
\end{figure}

\begin{figure}[H]
    \includegraphics[width=\textwidth]{screenshots/fmexp2_consent.png}
    \caption{Registration section}
    \label{screenshot:fmexp2_consent}
\end{figure}

\chapter*{Eidesstattliche Erklärung}

\vspace{2cm}

Hiermit versichere ich an Eides statt, dass ich die vorliegende Arbeit im
Bachelorstudiengang Informatik selbstständig verfasst und keine
anderen als die angegebenen Hilfsmittel – insbesondere keine im
Quellenverzeichnis nicht benannten Internet-Quellen – benutzt habe.
Alle Stellen, die wörtlich oder sinngemäß aus Veröffentlichungen
entnommen wurden, sind als solche kenntlich gemacht. Ich versichere
weiterhin, dass ich die Arbeit vorher nicht in einem anderen
Prüfungsverfahren eingereicht habe und die eingereichte schriftliche
Fassung der elektronischen Abgabe entspricht.

\vspace{5cm}

\newcommand*{\SignatureAndDate}[1]{%
    \par\noindent\makebox[2.5in]{\hrulefill} \hfill\makebox[2.0in]{\hrulefill}%
    \par\noindent\makebox[2.5in][l]{#1}      \hfill\makebox[2.0in][l]{Date}%
}%

\SignatureAndDate{Matz-Jona Radloff}

\end{document}
